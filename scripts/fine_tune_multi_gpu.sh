#!/bin/bash
#SBATCH --job-name=fine_tune_multi_gpu
#SBATCH --partition=batch
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=32
#SBATCH --mem=256G
#SBATCH --gpus=4                        # Same 4 GPUs as original
#SBATCH --time=12:00:00                 # Shorter time for fine-tuning
#SBATCH --output=logs/fine_tune_multi_gpu_%j.out
#SBATCH --error=logs/fine_tune_multi_gpu_%j.err

echo "ğŸ”„ FINE-TUNING FROM 72% DICE CHECKPOINT"
echo "======================================"
echo "ğŸ¯ STRATEGY:"
echo "  ğŸ“š Load: callbacks/multi_gpu_advanced_sota_20250624_044219/best_model.h5"
echo "  ğŸ”§ Method: Lower LR + Stronger regularization"
echo "  ğŸ¯ Target: Validation Dice 45% â†’ 55-65%"
echo "  â±ï¸ Duration: 6-12 hours (vs 30-40 for full training)"
echo ""
echo "Job ID: $SLURM_JOB_ID"
echo "Node: $SLURM_NODELIST"
echo "Start: $(date)"
echo ""

cd /mnt/beegfs/hellgate/home/rb194958e/stroke_segmentation_sota

# Enhanced environment setup (same as original)
module load gcc/9.3.0-5wu3 cuda/12.6.3-ziu7
eval "$(conda shell.bash hook)" || true
conda activate tf215_env

# Multi-GPU environment variables (same as original)
export LD_LIBRARY_PATH="/mnt/beegfs/hellgate/home/rb194958e/.conda/envs/tf215_env/lib:$LD_LIBRARY_PATH"
export TF_ENABLE_ONEDNN_OPTS=0
export TF_GPU_ALLOCATOR=cuda_malloc_async
export NIBABEL_NIFTI1_QFAC_CHECK=0
export TF_FORCE_GPU_ALLOW_GROWTH=true
export CUDA_VISIBLE_DEVICES=0,1,2,3
export NCCL_DEBUG=INFO

echo "ğŸ” CHECKPOINT STATUS:"
echo "Target checkpoint: callbacks/multi_gpu_advanced_sota_20250624_044219/best_model.h5"
if [ -f "callbacks/multi_gpu_advanced_sota_20250624_044219/best_model.h5" ]; then
    ls -lah callbacks/multi_gpu_advanced_sota_20250624_044219/best_model.h5
    echo "âœ… Checkpoint found and ready to load"
else
    echo "âŒ Checkpoint not found!"
    echo "Available checkpoints:"
    find callbacks/ -name "best_model.h5" -exec ls -lah {} \;
fi
echo ""

echo "ğŸ” MULTI-GPU ENVIRONMENT STATUS:"
nvidia-smi --query-gpu=index,name,memory.total,memory.free --format=csv
echo ""

echo "ğŸ§ª TESTING TENSORFLOW MULTI-GPU:"
python -c "
import tensorflow as tf
print(f'TensorFlow version: {tf.__version__}')
gpus = tf.config.list_physical_devices('GPU')
print(f'Detected GPUs: {len(gpus)}')
strategy = tf.distribute.MirroredStrategy()
print(f'âœ… MirroredStrategy ready with {strategy.num_replicas_in_sync} devices')
"
echo ""

echo "ğŸ“Š FINE-TUNING CONFIGURATION:"
echo "  ğŸ”„ Load checkpoint: 72% training Dice model"
echo "  ğŸ“‰ Learning rate: 5e-6 (down from 3e-4)"
echo "  âš–ï¸ Weight decay: 1e-4 (L2 regularization)"
echo "  ğŸ›‘ Early stopping: 12 epochs patience"
echo "  ğŸ² Enhanced augmentation: Mixup + elastic deformation"
echo "  ğŸ“ˆ Target improvement: 10-20% validation Dice"
echo ""

echo "ğŸ”„ STARTING FINE-TUNING..."
echo "Expected: Better generalization, reduced overfitting"
echo "Timeline: 6-12 hours for 30 epochs"
echo ""

# Run the fine-tuning
python -u fine_tune_multi_gpu.py

exit_code=$?

echo ""
echo "================================================================"
echo "ğŸ FINE-TUNING COMPLETED"
echo "================================================================"
echo "Exit code: $exit_code"
echo "End time: $(date)"
echo ""

if [ $exit_code -eq 0 ]; then
    echo "ğŸ‰ FINE-TUNING SUCCESSFUL!"
    echo ""
    echo "ğŸ† EXPECTED ACHIEVEMENTS:"
    echo "  ğŸ“ˆ Improved validation Dice (target: 55-65%)"
    echo "  ğŸ¯ Reduced train/validation gap"
    echo "  ğŸ§  Better generalization"
    echo "  ğŸ’ª More robust predictions"
    echo ""
    echo "ğŸ” CHECK RESULTS:"
    echo "  ğŸ“ˆ Training logs: logs/fine_tune_multi_gpu.log"
    echo "  ğŸ’¾ Best model: callbacks/fine_tune_*/best_model.h5"
    echo "  ğŸ“Š CSV logs: callbacks/fine_tune_*/training_log.csv"
    echo ""
    echo "ğŸ“Š PERFORMANCE COMPARISON:"
    echo "  Original training: 72% Dice"
    echo "  Original validation: 45% Dice"
    echo "  Fine-tuned validation: Check logs for improvement!"
    
else
    echo "âŒ FINE-TUNING FAILED"
    echo ""
    echo "ğŸ”§ DEBUGGING STEPS:"
    echo "  1. Check if checkpoint exists and is loadable"
    echo "  2. Verify custom_objects in model loading"
    echo "  3. Check GPU memory usage"
    echo ""
    echo "ğŸ’¡ FALLBACK OPTIONS:"
    echo "  - Try different checkpoint path"
    echo "  - Use single GPU fine-tuning"
    echo "  - Reduce batch size to 2"
fi

echo ""
echo "ğŸ“Š FINAL GPU STATUS:"
nvidia-smi --query-gpu=index,memory.used,memory.total,utilization.gpu --format=csv

echo ""
echo "ğŸ”„ FINE-TUNING SESSION COMPLETE!"
echo "================================================================"
