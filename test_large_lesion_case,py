#!/usr/bin/env python3
"""
Test the existing model on a large test lesion case
This will tell us if the model works when lesion size matches training data
"""

import os
import sys
import gc
import numpy as np
import nibabel as nib
import tensorflow as tf
from tensorflow.keras.models import load_model
from scipy.ndimage import zoom

# Custom imports
sys.path.append('/mnt/beegfs/hellgate/home/rb194958e/stroke_segmentation_sota')
from models.losses import dice_loss, combined_loss, focal_loss

def setup_tensorflow():
    """Setup TensorFlow"""
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        tf.config.set_visible_devices(gpus[0], 'GPU')
        tf.config.experimental.set_memory_growth(gpus[0], True)
    
    policy = tf.keras.mixed_precision.Policy('mixed_float16')
    tf.keras.mixed_precision.set_global_policy(policy)
    print(f"‚úÖ TensorFlow configured")

def load_fixed_model():
    """Load the fixed model"""
    model_path = "/mnt/beegfs/hellgate/home/rb194958e/stroke_segmentation_sota/callbacks/sota_fixed_20250619_063330/best_model.h5"
    
    def combined_loss_fn(y_true, y_pred, smooth=1e-6, focal_gamma=3.0, focal_alpha=0.25):
        y_true = tf.cast(y_true, tf.float32)
        y_pred = tf.cast(y_pred, tf.float32)
        y_true_f = tf.keras.backend.flatten(y_true)
        y_pred_f = tf.keras.backend.flatten(y_pred)
        intersection = tf.reduce_sum(y_true_f * y_pred_f)
        dice_loss = 1 - (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)
        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())
        p_t = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)
        alpha_t = tf.where(tf.equal(y_true, 1), focal_alpha, 1 - focal_alpha)
        focal_loss = -alpha_t * tf.pow(1 - p_t, focal_gamma) * tf.math.log(p_t)
        focal_loss = tf.reduce_mean(focal_loss)
        return 0.7 * dice_loss + 0.3 * focal_loss
    
    def dice_coeff_fn(y_true, y_pred, smooth=1e-6):
        y_true_f = tf.keras.backend.flatten(tf.cast(y_true, tf.float32))
        y_pred_f = tf.keras.backend.flatten(tf.cast(y_pred, tf.float32))
        intersection = tf.reduce_sum(y_true_f * y_pred_f)
        return (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)
    
    def binary_dice_fn(y_true, y_pred, smooth=1e-6):
        y_true = tf.cast(y_true, tf.float32)
        y_pred = tf.cast(y_pred > 0.5, tf.float32)
        y_true_f = tf.keras.backend.flatten(y_true)
        y_pred_f = tf.keras.backend.flatten(y_pred)
        intersection = tf.reduce_sum(y_true_f * y_pred_f)
        return (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)
    
    custom_objects = {
        'combined_loss': combined_loss_fn,
        'dice_coefficient': dice_coeff_fn,
        'binary_dice_coefficient': binary_dice_fn,
        'dice_loss': dice_loss,
        'focal_loss': focal_loss
    }
    
    model = load_model(model_path, custom_objects=custom_objects, compile=False)
    print(f"‚úÖ Model loaded: {model.count_params():,} parameters")
    return model

def resize_volume(volume, target_shape, order=1):
    """Resize volume"""
    factors = [t / s for t, s in zip(target_shape, volume.shape)]
    return zoom(volume, factors, order=order, mode='constant', cval=0)

def preprocess_image(image_data, target_shape=(192, 224, 176)):
    """Z-score preprocessing (best from previous tests)"""
    if image_data.shape != target_shape:
        image_data = resize_volume(image_data, target_shape, order=1)
    
    image_data = image_data.astype(np.float32)
    
    # Z-score normalization on non-zero voxels
    non_zero_mask = image_data > 0
    if np.any(non_zero_mask):
        mean_val = image_data[non_zero_mask].mean()
        std_val = image_data[non_zero_mask].std()
        image_data[non_zero_mask] = (image_data[non_zero_mask] - mean_val) / (std_val + 1e-8)
        image_data = np.clip(image_data, -3, 3)
        image_data = (image_data + 3) / 6
    
    return image_data[..., np.newaxis]

def test_case_detailed(model, case_name, test_dir):
    """Test model on a specific case with detailed analysis"""
    print(f"\n{'='*80}")
    print(f"DETAILED TESTING: {case_name}")
    print(f"{'='*80}")
    
    # Load data
    image_path = f"{test_dir}/Images/{case_name}_space-MNI152NLin2009aSym_T1w.nii.gz"
    mask_path = f"{test_dir}/Masks/{case_name}_space-MNI152NLin2009aSym_label-L_desc-T1lesion_mask.nii.gz"
    
    if not os.path.exists(image_path) or not os.path.exists(mask_path):
        print(f"‚ùå Files not found for {case_name}")
        return None
    
    nii_img = nib.load(image_path)
    image_data = nii_img.get_fdata(dtype=np.float32)
    true_mask = nib.load(mask_path).get_fdata().astype(np.uint8)
    original_shape = image_data.shape
    
    # Analyze lesion characteristics
    lesion_voxels = np.sum(true_mask)
    lesion_coords = np.array(np.where(true_mask))
    lesion_center = lesion_coords.mean(axis=1) if len(lesion_coords[0]) > 0 else [0, 0, 0]
    
    print(f"üìä LESION CHARACTERISTICS:")
    print(f"  Size: {lesion_voxels:,} voxels")
    print(f"  Center: [{lesion_center[0]:.1f}, {lesion_center[1]:.1f}, {lesion_center[2]:.1f}]")
    print(f"  Original shape: {original_shape}")
    
    # Compare to training data pattern
    train_center = [143.1, 140.3, 94.4]  # From our earlier analysis
    train_size = 112451
    
    center_distance = np.linalg.norm(np.array(lesion_center) - np.array(train_center))
    size_ratio = lesion_voxels / train_size
    
    print(f"\nüìê COMPARISON TO TRAINING DATA:")
    print(f"  Center distance from training: {center_distance:.1f} voxels")
    print(f"  Size ratio to training: {size_ratio:.3f}x")
    
    if center_distance < 30 and 0.5 < size_ratio < 2.0:
        print(f"  ‚úÖ Similar to training data - model should work well")
        expected_performance = "Good (Dice > 0.5)"
    elif center_distance < 50 and 0.1 < size_ratio < 5.0:
        print(f"  ‚ö†Ô∏è Somewhat similar to training data")
        expected_performance = "Moderate (Dice 0.2-0.5)"
    else:
        print(f"  ‚ùå Very different from training data")
        expected_performance = "Poor (Dice < 0.2)"
    
    print(f"  Expected performance: {expected_performance}")
    
    # Preprocess and predict
    print(f"\nüîß PREPROCESSING & PREDICTION:")
    processed = preprocess_image(image_data.copy())
    batch = processed[np.newaxis, ...]
    
    with tf.device('/GPU:0'):
        prediction = model(batch, training=False)
    
    pred_volume = prediction[0, :, :, :, 0].numpy()
    
    print(f"  Raw prediction stats:")
    print(f"    Max: {pred_volume.max():.6f}")
    print(f"    Mean: {pred_volume.mean():.6f}")
    print(f"    Values > 0.1: {np.sum(pred_volume > 0.1):,}")
    print(f"    Values > 0.5: {np.sum(pred_volume > 0.5):,}")
    
    # Resize prediction to original space
    if pred_volume.shape != original_shape:
        factors = [o / p for o, p in zip(original_shape, pred_volume.shape)]
        pred_resized = resize_volume(pred_volume, original_shape, order=1)
    else:
        pred_resized = pred_volume
    
    # Test multiple thresholds
    print(f"\nüéØ THRESHOLD TESTING:")
    thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
    
    best_dice = 0.0
    best_threshold = 0.5
    best_intersection = 0
    
    for threshold in thresholds:
        binary_pred = (pred_resized > threshold).astype(np.uint8)
        intersection = np.sum(binary_pred * true_mask)
        pred_voxels = np.sum(binary_pred)
        union = pred_voxels + lesion_voxels
        dice = (2.0 * intersection) / union if union > 0 else 0.0
        
        if dice > best_dice:
            best_dice = dice
            best_threshold = threshold
            best_intersection = intersection
        
        print(f"    Threshold {threshold:.1f}: Dice={dice:.6f}, Pred={pred_voxels:,}, Intersect={intersection:,}")
    
    print(f"\nüèÜ BEST RESULT:")
    print(f"  Dice: {best_dice:.6f}")
    print(f"  Threshold: {best_threshold:.1f}")
    print(f"  Intersection: {best_intersection:,} / {lesion_voxels:,} voxels")
    
    # Interpret results
    if best_dice > 0.5:
        interpretation = "‚úÖ EXCELLENT - Model works well on this case!"
    elif best_dice > 0.3:
        interpretation = "‚úÖ GOOD - Model shows reasonable performance"
    elif best_dice > 0.1:
        interpretation = "‚ö†Ô∏è MODERATE - Some detection but needs improvement"
    else:
        interpretation = "‚ùå POOR - Model fails on this case"
    
    print(f"\nüí° INTERPRETATION: {interpretation}")
    
    # Clean up
    del image_data, true_mask, processed, batch, prediction, pred_volume, pred_resized
    gc.collect()
    
    return {
        'case': case_name,
        'lesion_size': lesion_voxels,
        'lesion_center': lesion_center,
        'center_distance': center_distance,
        'size_ratio': size_ratio,
        'dice': best_dice,
        'threshold': best_threshold,
        'pred_max': pred_volume.max() if 'pred_volume' in locals() else 0,
        'interpretation': interpretation
    }

def main():
    """Test model on various test cases to understand performance patterns"""
    print(f"üîç COMPREHENSIVE TEST CASE ANALYSIS")
    print(f"=" * 80)
    print(f"Purpose: Test model on different types of test lesions")
    print(f"Strategy: Compare performance vs lesion size and location")
    
    setup_tensorflow()
    model = load_fixed_model()
    
    test_dir = "/mnt/beegfs/hellgate/home/rb194958e/Atlas_2/Testing_Split"
    
    # Test cases with different characteristics
    test_cases = [
        ("sub-r048s016_ses-1", "Large lesion (280k voxels)"),
        ("sub-r052s021_ses-1", "Medium-large lesion (75k voxels)"),
        ("sub-r048s018_ses-1", "Medium lesion (108k voxels)"),
        ("sub-r048s014_ses-1", "Small lesion (1k voxels)"),
        ("sub-r050s009_ses-1", "Tiny lesion (197 voxels)")
    ]
    
    results = []
    
    for case_name, description in test_cases:
        print(f"\n{'='*60}")
        print(f"TESTING: {description}")
        print(f"{'='*60}")
        
        result = test_case_detailed(model, case_name, test_dir)
        if result:
            results.append(result)
        
        # Clear memory
        tf.keras.backend.clear_session()
        gc.collect()
    
    # Summary analysis
    if results:
        print(f"\n{'='*80}")
        print(f"SUMMARY ANALYSIS")
        print(f"{'='*80}")
        
        print(f"üìä RESULTS BY LESION SIZE:")
        for result in sorted(results, key=lambda x: x['lesion_size'], reverse=True):
            print(f"  {result['case']:20s}: {result['lesion_size']:7,} voxels ‚Üí Dice={result['dice']:.4f}")
        
        # Find patterns
        large_lesions = [r for r in results if r['lesion_size'] > 50000]
        small_lesions = [r for r in results if r['lesion_size'] < 5000]
        
        if large_lesions:
            avg_large_dice = np.mean([r['dice'] for r in large_lesions])
            print(f"\nüìà LARGE LESIONS (>50k voxels): Average Dice = {avg_large_dice:.4f}")
        
        if small_lesions:
            avg_small_dice = np.mean([r['dice'] for r in small_lesions])
            print(f"üìâ SMALL LESIONS (<5k voxels): Average Dice = {avg_small_dice:.4f}")
        
        # Recommendation
        best_dice = max([r['dice'] for r in results])
        if best_dice > 0.3:
            print(f"\nüéâ GOOD NEWS: Model works on some test cases!")
            print(f"  ‚Üí Recommend retraining with mixed data to handle all lesion types")
        else:
            print(f"\n‚ùå MODEL STRUGGLES WITH ALL TEST CASES")
            print(f"  ‚Üí Need fundamental retraining or architecture changes")
    
    # Clean up
    del model
    tf.keras.backend.clear_session()
    gc.collect()
    
    return len([r for r in results if r['dice'] > 0.3]) > 0

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
